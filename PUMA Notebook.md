# Purple Martin (*Progne subis*) Population Genomic Study

**Robert R Fitak**; Department of Biology; Genomics and Bioinformatics Cluster, University of Central Florida

### Project Description

Whole-genome sequencing of 66 Purple Martins was performed as part of Stager et al. _In review_. "Immediate and long-term consequences of a storm-induced mass mortality event."



### Sample Collection

Purple Martin samples include 37 breeding individuals collected in May 2021 and 29 deceased individuals. Sample information is provided in Table S6.  



### DNA Extraction/Library Preparation/Sequencing

DNA was extracted from blood or muscle samples using Qiagen DNeasy Blood & Tissue Kit or New England Biolabs Monarch Spin gDNA Extraction Kit. Extracts were sent to Novogene for whole-genome library preparation and sequencing. Raw sequencing data for 66 samples were received at UCF. Raw sequences are deposited on NCBI's Sequence Read Archive (BioProject #).

*From Novogene's report summary:*

The genomic DNA was randomly sheared into shorter fragments. The obtained fragments were then end-repaired, A-tailed, and further ligated with Illumina adapters. The resulting fragments with adapters were size selected, PCR amplified unless otherwise specified as PCR-free, before proceeding for purification. The library was quantified through Qubit and qPCR, and size distribution detected with fragment analyzer. Quantified libraries were pooled and sequenced on Illumina platforms according to the effective library concentration and required data amount.

The sequenced reads (raw reads) often contain low quality reads and adapters, which will affect the analysis quality. So it's necessary to filter the raw reads and get the clean reads. The filtering process is as follows:

1. Remove reads containing adapters.
2. Remove reads containing N > 10% (N represents the base cannot be determined).
3. Remove reads containing low quality (Qscore<= 5) base which is over 50% of the total base.

***Sequences of adapter***

-  5' Adapter
  -  5'-AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT-3'
  -  This adapter is identical to Illumina "***PCR_Primer1_rc***" and is already in our adapter file list.
-  3' Adapter:
  -  5'-GATCGGAAGAGCACACGTCTGAACTCCAGTCACGGATGACTATCTCGTATGCCGTCTTCTGCTTG-3'
  -  The first half of this adapter is idential to Illumina's "***TruSeq3_Indexed Adapter***"/"***Illumina TruSeq Adapter Read 1***", but the entire sequence was added to our adapter list when trimming below.

### Bioinformatic Analyses

I have detailed all the bioinformatic analyses below.  Links to papers and/or specific programs are provided when necessary.  The version of the various software used is also provided.  Some analyses utilized custom, short scripts, and I have tried to provide them when needed.  All analyses were run on the *coombs* computer cluster at UCF unless otherwise stated.  *Coombs* is a Linux server currently running **Ubuntu 20.04.6 LTS**.



######  *Step 1: Inspect the raw data*

The sequencing data were generated by Novogene Inc., as part of project: **Davis-US-UCF-66-Purple martin-WGS-27G-WOBI-NVUS2023080834**. The final report summarizing all the sequencing can be found separately in the file ***Report_X202SC23082098-Z01-F001_20240205142933.zip***. The total output of data on the sequencer was 2045.0 G.  The raw, short-read sequencing data was produced on an Illumina Novoseq 6000 (Bob hasn't confirmed this).

The raw data included both forward and reverse read files for each of 66 samples, totaling 132 files.  The raw data files were confirmed to be uncorrupted by checking the md5 tags as follows (all files were confirmed "**OK**"):

``` bash
# Check MD5 tags
md5sum -c usftp21.novogene.com
   # usftp21.novogene.com is the folder from novogene containing all the raw data.
```



A file called `samples.txt` was generated to store a list of all 66 purple martin sample names:

```bash
# Generate list of sample names for use later
ls usftp21.novogene.com/01.RawData > samples.txt
```



######  *Step 2: Prepare the Progne subis reference genome*

The reference genome was generated by [de Greef et al. 2023](https://doi.org/10.1038/s41598-023-29470-7) as part of a resequencing study focusing on their migration.  The reference genome can be found in NCBI under accession [GCA_022316685.1](https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_022316685.1/).

```bash
# Download the reference genome from NCBI's FTP site
wget \
https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/022/316/685/GCA_022316685.1_UM_PSub_1/GCA_022316685.1_UM_PSub_1_genomic.fna.gz

# Uncompress the file and change its name
gunzip GCA_022316685.1_UM_PSub_1_genomic.fna.gz
mv GCA_022316685.1_UM_PSub_1_genomic.fna Psubis.fa

# Report basic sequencing statistics
seqstats Psubis.fa
   #   Total n:	2896
   #   Total seq:	1165951862 bp
   #   Avg. seq:	402607.69 bp
   #   Median seq:	77068.50 bp
   #   N 50:		6129949 bp
   #   Min seq:	16249 bp
   #   Max seq:	45082031 bp
   
# Index the reference genome
bwa index Psubis.fa
```



###### *Step 3: Clean and Map the Reads*

The reads were cleaned and mapped to the reference genome. The in-house pipeline, `supermapper` v5.3 was used for this process. `supermapper` cleans all the reads (including adapter removal) using [*fastp v0.20.0*](https://github.com/OpenGene/fastp) ([Chen et al. 2018](https://doi.org/10.1093/bioinformatics/bty560)) then maps then to the reference using [*bwa*](https://bio-bwa.sourceforge.net). The resulting BAM files are sorted, duplicates removed, then conservatively cleaned to remove improper reads. Once mapping was complete, the summary stats were generated from the resulting *BAM* files (`${name}.bamstats`). Additional details of various trimming and cleaning parameters are detailed below the code. The `supermapper` command is below, then the additional details within each step of the `supermapper` pipeline are detailed afterwards.

```bash
# Run supermapper (${name} references each of the samples in samples.txt)
name="TX_234"
supermapper \
	-i ${name}_1.fq.gz \
	-j ${name}_2.fq.gz \
	-r Psubis.fa \
	-c \
	-t 16 \
	-g "@RG\tID:${name}\tSM:${name}\tPL:illumina\tLB:run1" \
	-o $name
```



_clean the raw reads using fastp_

```bash
  # run Fastp
     # ${fwd} = forward reads file
     # ${rev} = reverse reads file
     # ${adapters} = fasta file of adapters to remove
     # ${out} = output file basename
     # ${threads} = # CPU threads to use
     # note that the results are piped to standard out
  fastp \
      --in1=${fwd} \
      --in2=${rev} \
      --stdout \
      --adapter_fasta=${adapters} \
      --cut_front \
      --cut_tail \
      --cut_window_size=4 \
      --cut_mean_quality=20 \
      --qualified_quality_phred=20 \
      --average_qual=20 \
      --unqualified_percent_limit=30 \
      --n_base_limit=5 \
      --length_required=50 \
      --low_complexity_filter \
      --complexity_threshold=30 \
      --overrepresentation_analysis \
      --trim_poly_x \
      --poly_x_min_len=10 \
      --html=${out}.html \
      --json=${out}.json \
      --report_title="$out" \
      --thread=${threads} | # piped directly into bwa
```

- _Parameters Explained:_
  - ***--in1/--in2*** :: input forward and reverse read files, recognizes gzip

  - ***--stdout*** :: write to standard out for piping

  - ***--adapter_fasta file*** :: a file of known Illumina adapters to trim

  - ***--cut_front*** :: enable a 5' sliding window trimmer, like trimmomatic

  - ***--cut_tail*** :: enable a 3' sliding window trimmer, like trimmomatic

  - ***--cut_window_size=4*** :: window size for the trimming

  - ***--cut_mean_quality=20*** :: mean base score across the window required, or else trim the last base

  - ***--qualified_quality_phred=20*** :: minimum base quality score to keep

  - ***--average_qual=20*** :: remove read of the average quality across all bases is < 20

  - ***--unqualified_percent_limit=30*** :: Percent of bases allowed to be less than q in a read

  - ***--n_base_limit=5*** :: if one read's number of N bases is >5, then this read pair is discarded

  - ***--length_required=50*** :: minimum read length to keep after trimming

  - ***--low_complexity_filter*** :: filter sequences with a low complexity

  - ***--complexity_threshold=30*** :: threshold for sequence complexity filter

  - ***--overrepresentation_analysis*** :: look for overrepresented sequences, like adapters

  - ***--trim_poly_x*** :: trim strings of homopolymers at the 3' end of reads

  - ***--poly_x_min_len 10*** :: minimum length of homopolymer ot trim

  - ***--json=${out}.json*** :: output file name, JSON format

  - ***--html=${out}.html*** :: output file name, HTML format

  - ***--report_title="$out"*** :: output report tile

  - ***--thread=${threads}***  :: number of cpus to use



_mapping with bwa mem_

```bash
# run bwa mem
   # ${threads} = # CPU threads to use
   # Psubis.fa = indexed reference genome from above
bwa mem \
      -M \
      -p \
      -R "$rg" \
      -t ${threads} \
      Psubis.fa \
      - | # read input piped from fastp; then output piped into samtools
```

- _Parameters Explained:_

  - ***-M*** :: mark shorter split hits as secondary

  - ***-p*** :: smart pairing (ignoring in2.fq), in other words, data input are interleaved fastq

  - ***-r*** :: read groups ID line, in other words, sample name

  - ***-t*** :: number of cpus to use



_initial processing with samtools_

```bash
# Process with samtools
   # ${threads} = # CPU threads to use
   # ${out} = output file basename
samtools sort \
   -T ${out}.tmp \
   -n \
   -@ ${threads} \
   - | \
   samtools fixmate \
      -@ ${threads} \
      -m \
      - - | \
      samtools sort \
         -T ${out}.tmp2 \
         -O bam \
         -@ ${threads} \
         - | \
         samtools markdup \
             -T ${out}.tmp3 \
             -O bam \
             -@ ${threads} \
             - ${out}.sorted.bam
```

- _Parameters Explained:_
  - `sort -n` :: sort BAM file numerically
  - `fixmate -m` :: fixmates and add mate score tag
  - `markdup` :: mark PCR/optical duplicates for later removal.



_cleaning the BAM file_

```bash
# Clean up the bam file
samtools view \
      -b \
      -h \
      -q 20 \
      -f 0x2 \
      -F 0x4 \
      -F 0x8 \
      -F 0x400 \
      -@ ${threads} \
      -o ${out}.clean.sorted.bam \
      ${out}.sorted.bam
```

- _Parameters Explained:_

  - ***-b*** :: output BAM format

  - ***-h*** :: Include header in SAM output

  - ***-q*** :: remove reads with mapping quality < 20
  - ***-f 0x2*** :: keep reads mapped in proper pair
  - ***-F 0x4*** :: remove unmapped reads
  - ***-F 0x8*** :: remove read when its mate is unmapped
  - ***-F 0x400*** :: remove read when mate is mapped to the reverse strand, or not the primary alignment.



_post-process BAM file_

```bash
# Process BAM and get stats
   # ${threads} = # CPU threads to use
   # ${out} = output file basename
samtools index -@ ${threads} ${out}.clean.sorted.bam
samtools stats -@ ${threads} ${out}.clean.sorted.bam > ${out}.bamstats
```



_summarize all output in `data.summary.csv`_

```bash
# Printer table header
echo -e "ID,raw reads,raw bases,trimmed reads,trimmed bases,mapped reads,mapped bases,coverage" > data.summary.csv

# Loop to write to output for each bird sample
while read i
   do
   reads=$(grep -e "total reads:" -e "total bases:" ${i}/${i}.html | perl -ne '/>([0-9.]* [MG])/; print "$1\n"' | tr "\n" "," | perl -ne 'print "$_\n"')
   mapped=$(grep -e "reads mapped and paired:" -e "bases mapped:" ${i}/${i}.bamstats | cut -f3 | tr "\n" "," | perl -ne 'chomp(); @a=split /,/;$b = $a[1] / 1165951862; print "$_$b\n"')
   echo -e "${i},${reads}${mapped}" >> data.summary.csv
   done < samples.txt
```



###### Step 4: Call Variants

Variants were identified using a relatively standard `bcftools v1.20` [Danecek et al. 2021](https://doi.org/10.1093/gigascience/giab008) pipeline.

Note that bcftools mpileup ignores reads that are marked as duplicates.

```bash
# Make a list of the 66 BAM files
ls ${out}.clean.sorted.bam > bams.list

# Call variants
   # Since bcftools 1.12, the filtering tests changed to Z values.
   # ${threads} = # CPU threads to use
bcftools \
	mpileup \
	-C 50 \
	-q 20 \
	-Q 20 \
	--threads 16 \
	-Ou \
	--per-sample-mF \
  --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR \
	-f Psubis.fa \
	-b bams.list | \
	bcftools \
	   call \
	   --threads 16 \
	   -f GQ \
	   -mv -Ou | \
	bcftools \
	   filter \
	   --threads 16 \
	   -sFAIL -e'QUAL < 30 || INFO/MQ <= 30 || MEAN(FMT/DP) > 45 || MEAN(FMT/DP) < 5  || INFO/MQBZ < -9 || INFO/RPBZ < -5 || INFO/RPBZ > 5 || INFO/BQBZ < -5 || INFO/DP4[3]+INFO/DP4[4] <= 2' \
	   -g10 \
	   -G10 \
	   -Ov | \
	bgzip -c > Psubis.vcf.gz

# Index
bcftools index --threads ${threads} Psubis.vcf.gz

# Stats
bcftools stats Psubis.vcf.gz > Psubis.SNPs.stats

# Filter
bcftools view -f 'PASS' -O v Psubis.vcf.gz | bgzip -c > Psubis.filtered.vcf.gz
bcftools stats -f "PASS" Psubis.filtered.vcf.gz > Psubis.filtered.SNPs.stats
```

- _Parameters Explained:_

  - ***-C 50*** :: adjust mapping quality; recommended 50

  - ***-q 20*** :: skip alignments with mapQ smaller than 20
  - ***-Q 20*** :: skip bases with baseQ/BAQ smaller than 20
  - ***-Ou*** :: uncompressed BCF output
  - ***-f Psubis.fa*** :: indexed reference genome file
  - ***-b bams.list*** :: list of BAM files to include
  - ***--threads=${threads}*** :: number of cpus to use
  - ***--per-sample-mF*** :: Apply **-m** and **-F** thresholds per sample to increase sensitivity of calling
  - ***--annotate*** :: add these extra SNP annotation fields so we can filter on them later
  - ***-f GQ*** :: add the **genotype quality** field for later filtering.
  - ***-m*** :: alternative model for multiallelic and rare-variant calling (conflicts with -c)
  - ***-v*** :: output variant sites only
  
  - ***-g10*** :: filter SNPs within 10 base pairs of an indel
  - ***-G10*** :: filter clusters of indels separated by 10 or fewer base pairs allowing only one to pass
  - ***-O v*** :: output type v: uncompressed VCF
  - ***-sFAIL*** :: annotate FILTER column with "FAIL"
  - ***-e[filter]*** :: exclude sites for which the expression [filter] is true (see man page for details)
    - QUAL < 30 [SNP quality]
    - INFO/MQ <= 30 [mean mapping quality of SNP reads]
    - INFO/MQBZ < -9 [mapping quality strand bias Z score]
    - INFO/RPBZ < -5 [read position bias Z score]
    - INFO/RPBZ > 5 [read position bias Z score]
    - INFO/BQBZ < -5 [base quality bias Z score]
    - INFO/DP4[3]+INFO/DP4[4] <= 2 [at least 2 reads with the alternate base]
    - MEAN(FMT/DP) > 45 ||  MEAN(FMT/DP) < 5 [mean depth across samples set to between 1/3 and 3X the average of ~15X] 

Below are some summary statistics from the called SNPs after filtering. In total, **6,901,009** variants (**5,470,651** SNPs) were called. This is consistent with [de Greef et al. 2023](https://doi.org/10.1038/s41598-023-29470-7), who recovered 4.6 million SNPs after filtering in 87 individuals.  Note, that individual genotype filtering has not yet been performed.  This is often performed in `vcftools` or similar program.  After filtering individual genotypes, you may lose additional variants for low call rate. The transition:transversion (Ti/Tv) ratio is right on par for high-quality SNP calling.  In general (based on primarily humans and mammals), a genome-wide Ti/Tv ratio ~2.0–2.1 is expected, as there is a heavy bias for transitions as opposted to transversions despite the random expectation of Ti/Tv = 0.5.  See references such as [Wang et al. 2015](https://doi.org/10.1093%2Fbioinformatics%2Fbtu668) and [Guo et al. 2014](https://doi.org/10.1093/bib/bbt069) for additional information.

| Type                             | All Sites | After Filtering | Description                                                  |
| -------------------------------- | --------- | --------------- | ------------------------------------------------------------ |
| number of samples                | 66        | 66              | Number of individuals                                        |
| number of records                | 22500709  | 6901009         | number of data rows in the VCF                               |
| number of no-ALTs                | 0         | 0               | reference-only sites, ALT is either "." or identical to REF  |
| number of SNPs                   | 18916340  | 5470651         | number of rows with a SNP                                    |
| number of MNPs                   | 0         | 0               | number of rows with a MNP, such as CC>TT                     |
| number of indels                 | 3584369   | 1430358         | number of rows with an indel                                 |
| number of others                 | 0         | 0               | number of rows with other type, e.g. a symbolic allele ora complex substitution, such as ACT>TCGA |
| number of multiallelic sites     | 972776    | 200683          | number of rows with multiple alternate alleles               |
| number of multiallelic SNP sites | 173360    | 29713           | number of rows with multiple alternate alleles, all SNP      |
| Ti/Tv                            | 1.75      | 2.06            | Transition:Transversion ratio. >2 is good for most vertebrates |

### Pool-seq Analysis

The following analyses are for doing a comparative analysis with the purple martin genomic data published by [de Greef et al. 2022](https://doi.org/10.1038/s41598-023-29470-7). De Greef et al. sequenced 87 individuals from various locations, albeit at low coverage (~2X mean depth of coverage). This low-coverage sequencing make a direct comparison to our higher-coverage genomes challenging.  As a result, we elected to make comparisons at a population-level using Pool-seq.  Pool-seq normally functions by pooling DNA from a population of individuals prior to sequencing, but in our case we will do this *in silico*. Pool-seq is an accurate technique to infer allele frequencies in a population, but a loss of all individual genotype information ([Schlötterer et al. 2014](https://doi.org/10.1038/nrg3803)). 

###### *Step 1: Clean and Map the Reads*

The reads were cleaned and mapped to the reference genome as described above (see previous mapping code for full details). The in-house pipeline, `supermapper` v5.3 was used for this process, starting with the SRA accession number (`$SRR`) for each purple martin sample.

```bash
# Run supermapper (${SRR} references each of the samples in samples.txt)
SRR="SRR16505166"
supermapper \
	-a ${SRR} \
	-r Psubis.fa \
	-c \
	-t 12 \
	-g "@RG\tID:${SRR}\tSM:${SRR}\tPL:illumina\tLB:run1" \
	-o $SRR
```

_summarize all output in `data.summary.csv`_

```bash
echo -e "ID,raw reads,raw bases,trimmed reads,trimmed bases,mapped reads,mapped bases,coverage" > data.SRR.summary.csv

# Loop to write to output for each bird sample
# the file "samples.SRR.txt" contains each SRR number
while read SRR
   do
   reads=$(grep -e "total reads:" -e "total bases:" ${SRR}/${SRR}.html | perl -ne '/>([0-9.]* [MG])/; print "$1\n"' | tr "\n" "," | perl -ne 'print "$_\n"')
   mapped=$(grep -e "reads mapped and paired:" -e "bases mapped:" ${SRR}/${SRR}.bamstats | cut -f3 | tr "\n" "," | perl -ne 'chomp(); @a=split /,/;$b = $a[1] / 1165951862; print "$_$b\n"')
   echo -e "${SRR},${reads}${mapped}" >> data.SRR.summary.csv
   done < samples.SRR.txt
```

###### *Step 2: Downsample each bam to the same coverage*

The minimum coverage across all samples was 1.10X (SRR16505101). As a result, all Bam files were downsampled %FRAC to reach 1.1X depth of coverage.

```bash
# Get accession
SRR="SRR16505166"

# Get Fraction to downsample (FRAC)
FRAC=$(grep "${SRR}" data.SRR.summary.csv | \
	cut -f8 -d',' | perl -ne '$f = 1.10484072540552/$_; print "$f"')

# Save output in folder
cd BAMS/${SRR}

# Downsample BAM
samtools view \
	--bam \
	--subsample ${FRAC} \
	--subsample-seed 19 \
	--threads 12 \
	--output ${SRR}.downsampled.bam \
	${SRR}.clean.sorted.bam

samtools stats ${SRR}.downsampled.bam > ${SRR}.downsampled.bamstats
samtools index ${SRR}.downsampled.bam

```

###### *Step 3: Prepare the pooled BAM files*

Here I prepared the files of individuals to keep in each pool. Maria provided the list of individuals in each pool.  For brevity, I am showing them as separate columns below.

```bash
Before			During	After		lat25-30		lat35-40		lat40-45		lat50-55
SRR16505095	TX_215	TX_008	SRR16505095	SRR16505119	SRR16505077	SRR16505071
SRR16505096	TX_216	TX_009	SRR16505096	SRR16505139	SRR16505079	SRR16505072
SRR16505102	TX_217	TX_010	SRR16505102	SRR16505163	SRR16505080	SRR16505073
SRR16505136	TX_218	TX_011	SRR16505136	SRR16505164	SRR16505090	SRR16505088
SRR16505097	TX_220	TX_020	SRR16505097	SRR16505104	SRR16505099	SRR16505092
SRR16505098	TX_221	TX_021	SRR16505098	SRR16505103	SRR16505115	SRR16505094
SRR16505100	TX_222	TX_022	SRR16505100	SRR16505105	SRR16505117	SRR16505121
SRR16505101	TX_223	TX_023	SRR16505101	SRR16505109	SRR16505124	SRR16505128
SRR16505107	TX_224					SRR16505107	SRR16505111	SRR16505127	SRR16505132
SRR16505113	TX_225	TX_025	SRR16505113	SRR16505112	SRR16505129	SRR16505143
SRR16505118	TX_226					SRR16505118	SRR16505114	SRR16505130	SRR16505151
SRR16505146	TX_227	TX_027	SRR16505146	SRR16505120	SRR16505131	SRR16505152
SRR16505119	TX_228					SRR16505108	SRR16505123	SRR16505140	SRR16505153
SRR16505139	TX_229	TX_029	SRR16505116	SRR16505110	SRR16505157	SRR16505154
SRR16505163	TX_230	TX_032	SRR16505074	SRR16505150	SRR16505076	SRR16505155
SRR16505164	TX_231	TX_033	SRR16505078	SRR16505162	SRR16505106	SRR16505165
						TX_232	TX_034	SRR16505081							SRR16505091	SRR16505166
										TX_036	SRR16505082							SRR16505161	SRR16505075
						TX_234	TX_045	SRR16505083							SRR16505142	SRR16505093
						TX_235	TX_046	SRR16505084													SRR16505122
						TX_236	TX_047	SRR16505085													SRR16505125
						TX_237	TX_060	SRR16505086													SRR16505126
						TX_238	TX_061	SRR16505087			
						TX_239	TX_062	SRR16505089			
						TX_240	TX_073	SRR16505133			
						TX_241	TX_074	SRR16505134			
						TX_242	TX_075	SRR16505135			
						TX_243	TX_076	SRR16505137			
										TX_077	SRR16505138			
										TX_078	SRR16505141			
										TX_079	SRR16505144			
										TX_101	SRR16505148			
										TX_102	SRR16505149			
										TX_103	SRR16505156			
										TX_104	SRR16505158			
										TX_105	SRR16505159			
										TX_107	SRR16505160
```

*Note: In the above table, birds TX_024, TX_026, TX_028, TX_233, TX_244 were ecluded due to relatedness.*



Below is a quick example code to make a list of the bamfiles for each pool, starting from a file of just SRA accession numbers:

```bash
# Make list of BAM file paths, starting from SRR numbers (3 item example; "before.pool")
# SRR16505095
# SRR16505096
# SRR16505102
sed -i "s_.*_&/&.downsampled.bam_g" before.pool

# Output
# SRR16505095/SRR16505095.downsampled.bam
# SRR16505096/SRR16505096.downsampled.bam
# SRR16505102/SRR16505102.downsampled.bam
```

Now, the BAM files were merged as needed using `samtools v1.21`. 

```bash
pool="before"
# Make header file for the pool
RG=$(printf "@RG\\\tID:${pool}\\\tSM:${pool}\\\tPL:illumina\\\tLB:run1")

# Merge BAM
samtools merge \
	--threads 12 \
	-b ${pool}.pool \
	- | \
	samtools addreplacerg \
	   -r $RG \
	   --threads 12 \
	   -o ${pool}.bam -

samtools stats ${pool}.bam > ${pool}.bamstats
samtools index ${pool}.bam
```

The high coverage pools, `during` and `after` were then downsampled to 10% coverage to be consistent with all the low coverage pools (see table below).

```bash
# Get pool
pool="during"

# Get Fraction to downsample (FRAC)

FRAC=$(grep -e "bases mapped:" ${pool}.bamstats | cut -f3 | perl -ne 'chomp(); $a = 20 / ($_ / 1165951862); print "$a\n"')
   # turns out that 0.1 is the smalled downsampling that can be performed.

# Downsample BAM
samtools view \
	--bam \
	--subsample ${FRAC} \
	--subsample-seed 19 \
	--threads 12 \
	--output ${pool}.downsampled.bam \
	${pool}.bam

samtools stats ${pool}.downsampled.bam > ${pool}.downsampled.bamstats
samtools index ${pool}.downsampled.bam

```

Below is a table of the final stats for each merged (i.e. pooled) BAM file:

| Pool                 | N    | Reads         | Bases           | Coverage |
| -------------------- | ---- | ------------- | --------------- | -------- |
| Before               | 16   | 137,883,510   | 20,611,052,551  | 17.677   |
| During               | 27   | 2,082,728,839 | 310,924,722,269 | 266.670  |
| During (downsampled) | 27   | 217,192,519   | 32,426,543,182  | 27.811   |
| After                | 34   | 2,619,772,439 | 391,551,556,721 | 335.821  |
| After (downsampled)  | 34   | 270,491,579   | 40,425,732,729  | 34.672   |
| lat25-30             | 37   | 318,857,603   | 47,660,601,024  | 40.877   |
| lat35-40             | 16   | 137,905,873   | 20,611,794,625  | 17.678   |
| lat40-45             | 19   | 163,775,744   | 24,477,787,636  | 20.994   |
| lat50-55             | 22   | 189,582,039   | 28,339,192,191  | 24.306   |



###### *Step 4: Run the poolseq pipeline*

Now that all our pooled BAM files are prepared, we are ready to run the poolseq analysis pipeline.  We are using [popoolation2 v1.201](https://sourceforge.net/p/popoolation2/wiki/Main/) ([Kofler et al. 2011](https://doi.org/10.1093/bioinformatics/btr589)). For additional details, please see their review paper in *Nature Reviews Genetics* ([Schlötterer et al. 2014](https://doi.org/10.1093/bioinformatics/btr589)) or this useful tutorial by Joanna Griffiths in the ***MarineOmics*** group (https://marineomics.github.io/POP_03_poolseq.html).

Below, the mandatory parameter `max-coverage` was calculated as 2.5X the mean coverage across pools (see table above).

-  (17.677 + 27.811 + 34.672 + 40.877 + 17.678 + 20.994 + 24.306) / 7 * 2.5 = 65.720 = 66

```bash
# Build the pileup of the 7 pools
samtools mpileup -B \
	after.downsampled.bam \
	before.bam \
	during.downsampled.bam \
	lat25-30.bam \
	lat35-40.bam \
	lat40-45.bam \
	lat50-55.bam \
	> pools.pileup

# Generate synchronized file
   # this is the multithreaded, ultrafast version only found in v1.201 of popoolation2
java -ea -Xmx7g -jar popoolation2_1201/mpileup2sync.jar \
        --input pools.pileup \
        --output pools.sync \
        --fastq-type sanger \
        --min-qual 20 \
        --threads 32

# Calculate allele frequencies and differences
   # 2.5X the mean coverage (17.677+27.811+34.672+40.877+17.678+20.994+24.306)/7*2.5 = 65.720 = 66
perl popoolation2_1201/snp-frequency-diff.pl \
        --input pools.sync \
        --output-prefix pools \
        --min-count 2 \
        --min-coverage 5 \
        --max-coverage 66

# Calculate pairwise Fst values
perl popoolation2_1201/fst-sliding.pl \
        --input pools.sync \
        --output pools.fst \
        --min-count 2 \
        --min-coverage 5 \
        --max-coverage 66 \
        --min-covered-fraction 0.75 \
        --window-size 25000 \
        --step-size 25000 \
        --pool-size 68:32:54:74:32:38:44 \
        --suppress-noninformative

# Calculate exact test of allele frequency differences
perl popoolation2_1201/fisher-test.pl \
        --input pools.sync \
        --output pools.fet \
        --min-count 2 \
        --min-coverage 5 \
        --max-coverage 66 \
        --min-covered-fraction 0.75 \
        --window-size 25000 \
        --step-size 25000 \
        --window-summary-method multiply \
        --suppress-noninformative
```

_Parameters Explained:_

- ***--fastq-type sanger*** :: quality scores are encoded in the standard Sanger format
- ***--min-qual 20*** :: minimum base quality score to consider
- ***--threads 32*** :: use 32 threads (multithreaded)
- ***--min-count 2*** :: the minimum count of the minor allele. used for SNP identification. SNPs will be identified considering all populations simultanously.
- ***--min-coverage 5*** :: the minimum coverage; used for SNP identification, the coverage in ALL populations has to be higher or equal to this threshold, otherwise no SNP will be called.
- ***--max-coverage 66*** :: The maximum coverage; All populations are required to have coverages lower or equal than the maximum coverage; Mandatory; The maximum coverage may be provided as one of the following:
  -  '500' :: a maximum coverage of 500 will be used for all populations
  - '300,400,500' :: a maximum coverage of 300 will be used for the first population, a maximum coverage of 400 for the second population and so on...
  - '2%' :: the 2% highest coverages will be ignored, this value is independently estimated for every population
- ***--min-covered-fraction 0.75*** :: the minimum fraction of a window being between min-coverage and max-coverage in ALL populations
- ***--window-size 25000*** :: the size of the sliding window. Measured in "--window-unit"
- ***--step-size 25000*** :: the size of the sliding window steps. Measured in "--window-unit"
- ***--pool-size 68:32:54:74:32:38:44*** :: the size of the population pools; May be provided for each population individually; mandatory parameter. NOTE THAT THIS IS THE # OF CHROMOSOMES (2 x # individuals for diploids)
  - --pool-size 500 .. all populations have a pool size of 500
  - --pool-size 500:300:600 .. first population has a pool size of 500, the seccond of 300 etc; the number of pools has to fit with the number of populations provided in the file
- ***--window-summary-method multiply*** :: Specify the method by which the p-values of individual SNPs should be summarized; possible: geometricmean | median | multiply
- ***--suppress-noninformative*** :: Suppress output for windows with no SNPs or insufficient coverage

###### *Step 5: Updated/Alternative poolseq pipeline*

After completing the traditional *popoolation2* pipeline above, a quick literature search revealed that there is a new software for poolseq analysis, [grenedalf](https://github.com/lczech/grenedalf) ([Czech et al. 2024](https://doi.org/10.1093/bioinformatics/btae508)). This new software was developed specifically in conjunction with Robert Kofler, the author of both *popoolation* and *popoolation2*. The paper is a great read, as it is a thorough treatment of the population genetic parametes used in the sofware (i.e., $\pi$, F~ST~, Tajima's D, etc) and how their derivation needs to be modified for poolseq data. <u>It turns out that the Tajima's D cannot actually be properly derived for poolseq data, and any study published previously using this metric form pooled data must consider a re-analysis</u>:

> "In addition, we noticed several implementation bugs in POPOOLATION (Kofler *et al.* 2011a), up until and including v1.2.2 of the tool. We discussed these with the authors, and the bugs have since been fixed (pers. comm. with R. Kofler). If conclusions of studies depend on numerical values of Tajima’s *D*computed with POPOOLATION, we recommend reanalyzing the data. Due to these statistical issues, we generally advise to be cautious when applying and interpreting Tajima’s *D* with Pool-seq data."

Although our purple martin analysis is not using metrics of variation, and the F~ST~ implementation is generally unchanged, I still wanted to run this new pipeline on our pools as well. *grenedalf v0.6.2* was pretty easy to install and run, and actually much more user friendly and scalable than *popoolation2*. It also has an improved interface for defining windows and their denomination to decrease biases (read their paper and supplementary methods). I highly recommend it. This code is below.

```bash
# Get allele frequencies
grenedalf \
	frequency \
	--sam-path after.downsampled.bam \
	--sam-path before.bam \
	--sam-path during.downsampled.bam \
	--sam-path lat25-30.bam \
	--sam-path lat35-40.bam \
	--sam-path lat40-45.bam \
	--sam-path lat50-55.bam \
	--sam-min-map-qual 20 \
	--sam-min-base-qual 20 \
	--reference-genome-fasta Psubis.fa \
	--write-sample-counts \
	--write-sample-read-depth \
	--write-sample-ref-freq \
	--separator-char comma \
	--file-prefix grenedalf. \
	--verbose \
	--threads 48

# Get Fst metrics
grenedalf \
        fst \
        --sam-path after.downsampled.bam \
        --sam-path before.bam \
        --sam-path during.downsampled.bam \
        --sam-path lat25-30.bam \
        --sam-path lat35-40.bam \
        --sam-path lat40-45.bam \
        --sam-path lat50-55.bam \
        --sam-min-map-qual 20 \
        --sam-min-base-qual 20 \
        --reference-genome-fasta Psubis.fa \
        --filter-sample-min-count 2 \
        --filter-sample-max-count 0 \
        --filter-sample-min-read-depth 5 \
        --filter-sample-max-read-depth 66 \
        --filter-total-only-biallelic-snps \
        --window-type interval \
        --window-interval-width 25000 \
        --window-interval-stride 25000 \
        --window-average-policy valid-loci \
        --method unbiased-nei \
        --pool-sizes pool.sizes \
        --file-prefix grenedalf. \
        --verbose \
        --threads 48

# The file pool.sizes contains the chromosome count (2 x N) in each pool:
after.downsampled,68
before,32
during.downsampled,54
lat25-30,74
lat35-40,32
lat40-45,38
lat50-55,44
```

_Parameters Explained:_

- ***--sam-path file*** :: path to BAM file of mapped reads, cleaned of duplicates, poorly mapepd reads, etc.
- ***--sam-min-map-qual 20*** :: minimum mapping quality score to consider a read
- ***--sam-min-base-qual 20*** :: minimum base quality score to consider
- ***--reference-genome-fasta Psubis.fa*** :: fasta file of the reference genome
- ***--write-sample-counts*** :: write 'REF_CNT' and 'ALT_CNT' columns per sample, containing the REF and ALT base counts at the position for each sample
- ***--write-sample-read-depth*** :: write a 'DEPTH' column per sample, containing the read depth (sum of REF and ALT) counts of each sample.
- ***--write-sample-ref-freq*** :: write a 'FREQ' column per sample, containing the reference allele frequency, computed as REF/(REF+ALT) of the counts of each sample
- ***--separator-char comma*** :: Separator char between fields of output tabular data {comma, tab, space, semicolon}
- ***--filter-sample-min-count 2*** :: Minimum base count for a nucleotide (in `ACGT`) to be considered as an allele
- ***--filter-sample-max-count 0*** :: Maximum base count for a nucleotide (in `ACGT`) to be considered as an allele
- ***--filter-sample-min-read-depth 5*** :: Minimum read depth expected for a position in a sample to be considered covered. If the sum of nucleotide counts (in `ACGT`) at a given position in a sample is less than the provided value, the sample is ignored at this position.
- ***--filter-sample-max-read-depth 66*** :: Maximum read depth expected for a position in a sample to be considered covered. If the sum of nucleotide counts (in `ACGT`) at a given position in a sample is greater than the provided value, the sample is ignored at this position.
- ***--filter-total-only-biallelic-snps*** :: Filter out any positions that do not have exactly two alleles across all samples. That is, after applying all previous filters, if not exactly two counts (in `ACGT`) are non-zero in total across all samples, the position is not considered a biallelic SNP, and ignored.
- ***--window-type interval*** :: Type of window to use. Depending on the type, additional options might need to be provided.
  - `interval`: Typical sliding window over intervals of fixed length (in bases) along the genome
  - `queue`: Sliding window, but instead of using a fixed length of bases along the genome, it uses a fixed number of positions of the input data. Typically used for windowing over variant positions such as (biallelic) SNPs, and useful for example when SNPs are sparse in the genome.
  - `single`: Treat each position of the input data as an individual window of size 1. This is typically used to process single SNPs, and equivalent to `interval` or `queue` with a width/count of 1, except that positions that are removed by some filter are skipped.
  - `regions`: Windows corresponding to some regions of interest, such as genes. The regions need to be provided, and can be overlapping or nested as needed.
  - `chromosomes`: Each window covers a whole chromosome.
  - `genome`: The whole genome is treated as a single window.
- ***--window-interval-width 25000*** :: Required when using `--window-type interval`: Width of each window along the chromosome, in bases
- ***--window-interval-stride 25000*** :: When using `--window-type interval`: Stride between windows along the chromosome, that is how far to move to get to the next window
- ***--window-average-policy valid-loci*** :: Denominator to use when computing the average of a metric in a window:
  - `window-length`: Simply use the window length, which likely underestimates the metric, in particular in regions with low coverage and high missing data.
  - `available-loci`: Use the number of positions for which there was data at all (that is, absent or missing data is excluded), independent of all other filter settings.
  - `valid-loci`: Use the number of positions that passed all quality and numerical filters (that is, excluding the SNP-related filters). This uses all positions of high quality, and is the recommended policy when the input contains data for all positions.
  - `valid-snps`: Use the number of SNPs only. This might overestimate the metric, but can be useful when the data only consists of SNPs.
  - `sum`: Simply report the sum of the per-site values, with no averaging applied to it. This can be used to apply custom averaging later.
  - `provided-loci`: Use the exact loci provided via `--window-average-loci-bed` or `--window-average-loci-fasta` to determine the denominator for the window averaging, by counting all positions set in this mask in the given window.
- ***--method unbiased-nei*** :: FST method to use for the computation:
  - `unbiased-nei` or `unbiased-hudson` The unbiased pool-sequencing statistic (in two variants, following the definition of Nei, and the definition of Hudson)
  - `kofler` the statistic by Kofler et al of PoPoolation2
  - `karlsson` the asymptotically unbiased estimator of Karlsson et al (which is also implemented in *PoPoolation2*)
  - All except for the Karlsson method also require `--pool-sizes` to be provided.
- ***--pool-sizes pool.sizes*** :: Pool sizes for all samples that are used (not filtered out). These are the number of haploids, so 100 diploid individuals correspond to a pool size of 200. Either:
  - a single pool size that is used for all samples, specified on the command line, or
  - a path to a file that contains a comma- or tab-separated list of sample names and pool sizes, with one name/size pair per line, in any order of lines.
- **--file-prefix grenedalf**. :: File prefix for output files
- **--verbose** :: use verbose output reporting
- **--threads 48** :: number of parallel threads to use

_Poolseq Output Files_

| File Name               | Program        | Metric                                | Column Description                                           | Notes         |
| ----------------------- | -------------- | ------------------------------------- | ------------------------------------------------------------ | ------------- |
| pools_pwc               | *popoolation2* | Pairwise allele frequency differences | [Link](https://sourceforge.net/p/popoolation2/wiki/Manual/)  | Tab delimited |
| pools.fet               | *popoolation2* | Fisher's Exact Test                   | [Link](https://sourceforge.net/p/popoolation2/wiki/Manual/)  | Tab delimited |
| pools.fst               | *popoolation2* | Pairwise Fst values                   | [Link](https://sourceforge.net/p/popoolation2/wiki/Manual/)  | Tab delimited |
| grenedalf.frequency.csv | *grenedalf*    | Pairwise allele frequency comparisons | [Link](https://github.com/lczech/grenedalf/wiki/Subcommand:-frequency) | csv           |
| grenedalf.fst.csv       | *grenedalf*    | Pairwise Fst values                   | [Link](https://github.com/lczech/grenedalf/wiki/Output)      | csv           |

*Note for popoolation2  results the populations are numbered 1–7 in the order they appear in the mpileup:*

- population 1 = after
- population 2 = before
- population 3 = during
- population 4 = lat25-30
- population 5 = lat35-40
- population 6 = lat40-45
- population 7 = lat50-55
